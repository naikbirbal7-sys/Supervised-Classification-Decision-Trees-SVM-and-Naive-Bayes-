{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Classification: Decision\n",
        "Trees, SVM, and Naive Bayes|"
      ],
      "metadata": {
        "id": "ZMiaw7XUhZ9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: What is Information Gain, and how is it used in Decision Trees?  Answer: Information Gain is a metric used to train decision trees. It measures the reduction in entropy (uncertainty) after a dataset is split on an attribute. In Decision Trees, the feature with the highest Information Gain is chosen for the split because it provides the most \"information\" about the target class."
      ],
      "metadata": {
        "id": "Xvs0gTLHi2oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What is the difference between Gini Impurity and Entropy?  Answer:\n",
        "\n",
        "\n",
        "Gini Impurity: It measures the probability of a random element being misclassified. It is computationally faster as it doesn't use logarithms. Its range is 0 to 0.5.\n",
        "\n",
        "\n",
        "Entropy: It measures the disorder or randomness in the data. It involves logarithmic calculations, making it slightly slower. Its range is 0 to 1."
      ],
      "metadata": {
        "id": "hU-hcv3HjAmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is Pre-Pruning in Decision Trees?  Answer: Pre-pruning is a technique where the growth of a decision tree is stopped early to avoid Overfitting. This is done by setting constraints like max_depth, min_samples_split, or min_samples_leaf during the training process so the tree doesn't become too complex."
      ],
      "metadata": {
        "id": "edFamQXtjIeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Write a Python program to train a Decision Tree Classifier using Gini Impurity and print feature importances.  Answer:"
      ],
      "metadata": {
        "id": "AskJvBo_jLsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train model with Gini\n",
        "# [cite: 60, 110]\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "# [cite: 60, 110]\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsS-y59lj-DG",
        "outputId": "3ed54127-dd9d-48e6-e245-d1da8593cc59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: What is a Support Vector Machine (SVM)?  Answer: SVM is a supervised learning algorithm used for classification and regression. It works by finding the \"Hyperplane\" in an N-dimensional space that separates data points into different classes with the maximum possible margin."
      ],
      "metadata": {
        "id": "fJPIAPQPkBxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: What is the Kernel Trick in SVM? Answer: The Kernel Trick is a method used by SVM to handle non-linear data. It allows the algorithm to map data points from a low-dimensional space into a higher-dimensional space where they can be easily separated by a linear hyperplane. The best part is that it does this without the high computational cost of actual transformation."
      ],
      "metadata": {
        "id": "lTOjXiqlkE7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies. Answer (Code for your Colab):"
      ],
      "metadata": {
        "id": "7BNZpD5TkHB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data load aur split\n",
        "data = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "linear_model = SVC(kernel='linear').fit(X_train, y_train)\n",
        "lin_acc = accuracy_score(y_test, linear_model.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "rbf_model = SVC(kernel='rbf').fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_model.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {lin_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klt2NSnfkItI",
        "outputId": "ecacc518-f1b6-4609-fdc7-fd847cffc03b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9815\n",
            "RBF Kernel Accuracy: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"? Answer: Naïve Bayes is a classification algorithm based on Bayes' Theorem. It is called \"Naïve\" because it makes a very strong and simplified assumption that all features in the dataset are completely independent of each other. In real life, features are usually related, but this assumption makes the model very fast and effective for things like spam filtering."
      ],
      "metadata": {
        "id": "js-BS4uJkLJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes. Answer: * Gaussian NB: Used when features follow a normal distribution (continuous data like temperature or height).\n",
        "\n",
        "Multinomial NB: Used for discrete counts, like counting how many times a word appears in a document.\n",
        "\n",
        "Bernoulli NB: Used when features are binary (Yes/No or 0/1), like checking if a word is present or not."
      ],
      "metadata": {
        "id": "JdtheTxqkO26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. Answer (Code for your Colab):"
      ],
      "metadata": {
        "id": "LskLCN2LkRDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Data loading\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training Gaussian NB\n",
        "gnb = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "# Accuracy print karna\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(f\"Gaussian Naive Bayes Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmHO0wOhkSPM",
        "outputId": "39870f47-2aef-44eb-8eb4-0f668afabb7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9415\n"
          ]
        }
      ]
    }
  ]
}